import torch
from PIL import Image
import numpy as np
import os
from .qwen3_vl_manager import model_manager

# é¦–å…ˆæ›´æ–°æ¨¡å‹ç®¡ç†å™¨ä»¥ä½¿ç”¨æ­£ç¡®çš„æ¨¡å‹ç±»
class Qwen3VLModelManager:
    def __init__(self):
        self.model = None
        self.processor = None
        self.current_model_path = None

    def load_model(self, model_path):
        if os.path.isdir(model_path):
            print(f"âœ… æ£€æµ‹åˆ°æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼š{model_path}ï¼Œä¼˜å…ˆåŠ è½½æœ¬åœ°æ–‡ä»¶")
        else:
            print(f"âš ï¸ æœ¬åœ°è·¯å¾„ä¸å­˜åœ¨ï¼Œå°†å°è¯•è¿œç¨‹ä¸‹è½½ï¼š{model_path}")

        if self.current_model_path == model_path and self.model is not None:
            return self.model, self.processor

        # ä½¿ç”¨æ­£ç¡®çš„æ¨¡å‹ç±»å’Œå¤„ç†å™¨
        from transformers import Qwen3VLForConditionalGeneration, AutoProcessor
        
        # åŠ è½½å¤„ç†å™¨
        self.processor = AutoProcessor.from_pretrained(
            model_path,
            trust_remote_code=True,
            local_files_only=os.path.isdir(model_path)
        )
        
        # åŠ è½½æ¨¡å‹ - ä½¿ç”¨å®˜æ–¹æ¨èçš„æ–¹å¼
        self.model = Qwen3VLForConditionalGeneration.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True,
            local_files_only=os.path.isdir(model_path)
        )
        
        self.model.eval()
        self.current_model_path = model_path
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼š{model_path}")
        
        return self.model, self.processor

# æ›´æ–°å•ä¾‹æ¨¡å¼ç®¡ç†æ¨¡å‹
model_manager = Qwen3VLModelManager()

# çº¯æ–‡æœ¬å¯¹è¯èŠ‚ç‚¹
class Qwen3VL_TextChat:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "model_path": ("STRING", {"default": "Qwen/Qwen3-VL-4B-Instruct"}),
                "prompt": ("STRING", {"multiline": True, "default": "ä½ å¥½ï¼Œä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"}),
                "max_new_tokens": ("INT", {"default": 512, "min": 1, "max": 2048}),
                "show_thinking": ("BOOLEAN", {"default": True}),
            },
            "optional": {
                "history": ("LIST", {"default": []}),
            }
        }

    RETURN_TYPES = ("STRING", "STRING", "LIST")
    RETURN_NAMES = ("response", "thinking_process", "updated_history")
    FUNCTION = "chat"
    CATEGORY = "Qwen3-VL"

    def chat(self, model_path, prompt, max_new_tokens, show_thinking=True, history=None):
        """
        çº¯æ–‡æœ¬èŠå¤©æ–¹æ³• - åŸºäºå®˜æ–¹ç¤ºä¾‹ä¿®å¤
        """
        history = history or []
        model, processor = model_manager.load_model(model_path)
        
        print("ğŸš€ å¼€å§‹ç”Ÿæˆæ–‡æœ¬å›ç­”...")
        
        # æ„å»ºæç¤ºè¯
        if show_thinking:
            thinking_prompt = f"""è¯·å…ˆæ€è€ƒå†å›ç­”ï¼š
é—®é¢˜ï¼š{prompt}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š
æ€è€ƒï¼š<ä½ çš„æ€è€ƒè¿‡ç¨‹>
å›ç­”ï¼š<ä½ çš„æœ€ç»ˆå›ç­”>
"""
        else:
            thinking_prompt = prompt
        
        # ä¿®æ”¹ä¸ºï¼ˆæ­£ç¡®ä»£ç ï¼‰
        messages = [
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": thinking_prompt
                    }
                ]
            }
        ]

        
        try:
            # ä½¿ç”¨å®˜æ–¹ç¤ºä¾‹çš„æ–¹æ³•
            inputs = processor.apply_chat_template(
                messages,
                tokenize=True,
                add_generation_prompt=True,
                return_dict=True,
                return_tensors="pt"
            )
            inputs = inputs.to(model.device)
            
            with torch.no_grad():
                generated_ids = model.generate(
                    **inputs, 
                    max_new_tokens=max_new_tokens
                )
            
            # å®˜æ–¹æ¨èçš„è§£ç æ–¹å¼
            generated_ids_trimmed = [
                out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
            ]
            output_text = processor.batch_decode(
                generated_ids_trimmed, 
                skip_special_tokens=True, 
                clean_up_tokenization_spaces=False
            )
            
            full_response = output_text[0] if output_text else ""
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return ("ç”Ÿæˆå¤±è´¥", f"ç”Ÿæˆé”™è¯¯: {str(e)}", history)
        
        # å°è¯•åˆ†ç¦»æ€è€ƒè¿‡ç¨‹å’Œæœ€ç»ˆå›ç­”
        thinking_process = ""
        final_answer = full_response
        
        if show_thinking and "æ€è€ƒï¼š" in full_response and "å›ç­”ï¼š" in full_response:
            # åˆ†ç¦»æ€è€ƒè¿‡ç¨‹å’Œæœ€ç»ˆå›ç­”
            thinking_start = full_response.find("æ€è€ƒï¼š")
            answer_start = full_response.find("å›ç­”ï¼š")
            
            if thinking_start < answer_start:
                thinking_process = full_response[thinking_start + 3:answer_start].strip()
                final_answer = full_response[answer_start + 3:].strip()
                
                print(f"ğŸ¤” æ€è€ƒè¿‡ç¨‹ï¼š{thinking_process}")
                print(f"ğŸ’¡ æœ€ç»ˆå›ç­”ï¼š{final_answer}")
        elif show_thinking:
            thinking_process = "æ¨¡å‹æœªæŒ‰æŒ‡å®šæ ¼å¼æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹"
            print(f"ğŸ’¡ å›ç­”ï¼š{final_answer}")
        else:
            thinking_process = "æ€è€ƒè¿‡ç¨‹å·²éšè—"
            print(f"ğŸ’¡ å›ç­”ï¼š{final_answer}")
        
        updated_history = history + [
            {"role": "user", "content": prompt},
            {"role": "assistant", "content": final_answer}
        ]
        
        print("âœ… æ–‡æœ¬ç”Ÿæˆå®Œæˆï¼")
        
        return (final_answer, thinking_process, updated_history)

# å¤šæ¨¡æ€å¯¹è¯èŠ‚ç‚¹
class Qwen3VL_MultimodalChat:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "model_path": ("STRING", {"default": "Qwen/Qwen3-VL-4B-Instruct"}),
                "prompt": ("STRING", {"multiline": True, "default": "è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹"}),
                "max_new_tokens": ("INT", {"default": 512, "min": 1, "max": 2048}),
                "show_thinking": ("BOOLEAN", {"default": True}),
                "image": ("IMAGE",),
            },
            "optional": {
                "history": ("LIST", {"default": []}),
            }
        }

    RETURN_TYPES = ("STRING", "STRING", "LIST")
    RETURN_NAMES = ("response", "thinking_process", "updated_history")
    FUNCTION = "chat"
    CATEGORY = "Qwen3-VL"

    def chat(self, model_path, prompt, max_new_tokens, show_thinking=True, image=None, history=None):
        """
        å¤šæ¨¡æ€èŠå¤©æ–¹æ³• - åŸºäºå®˜æ–¹ç¤ºä¾‹å®Œå…¨é‡å†™
        """
        history = history or []
        model, processor = model_manager.load_model(model_path)
        
        # å¤„ç†å›¾åƒ
        processed_image = None
        if image is not None:
            try:
                # æ­£ç¡®å¤„ç†æ‰¹æ¬¡å›¾åƒ
                if len(image.shape) == 4:  # [batch, height, width, channels]
                    img_tensor = image[0]  # å–ç¬¬ä¸€å¼ å›¾ç‰‡
                else:
                    img_tensor = image
                
                # ç¡®ä¿æ•°å€¼èŒƒå›´æ­£ç¡®å¹¶è½¬æ¢ä¸ºPILå›¾åƒ
                if img_tensor.max() <= 1.0:
                    img_np = (img_tensor.cpu().numpy() * 255).astype(np.uint8)
                else:
                    img_np = img_tensor.cpu().numpy().astype(np.uint8)
                
                # å¤„ç†é€šé“ç»´åº¦
                if len(img_np.shape) == 3 and img_np.shape[-1] == 3:
                    processed_image = Image.fromarray(img_np)
                else:
                    # å¦‚æœæ˜¯å•é€šé“æˆ–å…¶ä»–æ ¼å¼ï¼Œè½¬æ¢ä¸ºRGB
                    processed_image = Image.fromarray(img_np.squeeze()).convert('RGB')
                
                print(f"ğŸ–¼ï¸ æˆåŠŸå¤„ç†å›¾åƒè¾“å…¥ï¼Œå°ºå¯¸: {processed_image.size}")
            except Exception as e:
                print(f"âš ï¸ å¤„ç†å›¾åƒæ—¶å‡ºé”™: {e}")
                import traceback
                print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
                return ("å›¾åƒå¤„ç†å¤±è´¥", f"å›¾åƒå¤„ç†é”™è¯¯: {str(e)}", history)
        else:
            return ("é”™è¯¯", "æœªæä¾›å›¾åƒè¾“å…¥", history)
        
        print("ğŸš€ å¼€å§‹ç”Ÿæˆå¤šæ¨¡æ€å›ç­”...")
        
        # æ„å»ºå¤šæ¨¡æ€æ¶ˆæ¯ - å®Œå…¨æŒ‰ç…§å®˜æ–¹æ ¼å¼
        if show_thinking:
            thinking_prompt = f"""è¯·å…ˆè§‚å¯Ÿå›¾ç‰‡å¹¶æ€è€ƒï¼Œç„¶åå›ç­”ï¼š
é—®é¢˜ï¼š{prompt}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š
æ€è€ƒï¼š<ä½ çš„è§‚å¯Ÿå’Œæ€è€ƒè¿‡ç¨‹>
å›ç­”ï¼š<ä½ çš„æœ€ç»ˆå›ç­”>
"""
        else:
            thinking_prompt = prompt
        
        try:
            # ä½¿ç”¨å®˜æ–¹ç¤ºä¾‹çš„å¤šæ¨¡æ€æ¶ˆæ¯æ ¼å¼
            messages = [
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "image",
                            "image": processed_image,  # ç›´æ¥ä½¿ç”¨PILå›¾åƒå¯¹è±¡
                        },
                        {
                            "type": "text", 
                            "text": thinking_prompt
                        }
                    ]
                }
            ]
            
            # ä½¿ç”¨å®˜æ–¹ç¤ºä¾‹çš„é¢„å¤„ç†æ–¹æ³•
            inputs = processor.apply_chat_template(
                messages,
                tokenize=True,
                add_generation_prompt=True,
                return_dict=True,
                return_tensors="pt"
            )
            inputs = inputs.to(model.device)
            
            print("âœ… æˆåŠŸé¢„å¤„ç†å¤šæ¨¡æ€è¾“å…¥")
            
        except Exception as e:
            print(f"âš ï¸ æ¶ˆæ¯é¢„å¤„ç†å¤±è´¥: {e}")
            import traceback
            print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return ("æ¶ˆæ¯é¢„å¤„ç†å¤±è´¥", f"é¢„å¤„ç†é”™è¯¯: {str(e)}", history)
        
        # ç”Ÿæˆå›ç­”
        try:
            with torch.no_grad():
                generated_ids = model.generate(
                    **inputs, 
                    max_new_tokens=max_new_tokens
                )
            
            # å®˜æ–¹æ¨èçš„è§£ç æ–¹å¼ - è·³è¿‡è¾“å…¥éƒ¨åˆ†
            generated_ids_trimmed = [
                out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
            ]
            output_text = processor.batch_decode(
                generated_ids_trimmed, 
                skip_special_tokens=True, 
                clean_up_tokenization_spaces=False
            )
            
            full_response = output_text[0] if output_text else ""
            
            print("âœ… å¤šæ¨¡æ€ç”ŸæˆæˆåŠŸï¼")
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return ("ç”Ÿæˆå¤±è´¥", f"ç”Ÿæˆé”™è¯¯: {str(e)}", history)
        
        # å°è¯•åˆ†ç¦»æ€è€ƒè¿‡ç¨‹å’Œæœ€ç»ˆå›ç­”
        thinking_process = ""
        final_answer = full_response
        
        if show_thinking and "æ€è€ƒï¼š" in full_response and "å›ç­”ï¼š" in full_response:
            # åˆ†ç¦»æ€è€ƒè¿‡ç¨‹å’Œæœ€ç»ˆå›ç­”
            thinking_start = full_response.find("æ€è€ƒï¼š")
            answer_start = full_response.find("å›ç­”ï¼š")
            
            if thinking_start < answer_start:
                thinking_process = full_response[thinking_start + 3:answer_start].strip()
                final_answer = full_response[answer_start + 3:].strip()
                
                print(f"ğŸ¤” æ€è€ƒè¿‡ç¨‹ï¼š{thinking_process}")
                print(f"ğŸ’¡ æœ€ç»ˆå›ç­”ï¼š{final_answer}")
        elif show_thinking:
            thinking_process = "æ¨¡å‹æœªæŒ‰æŒ‡å®šæ ¼å¼æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹"
            print(f"ğŸ’¡ å›ç­”ï¼š{final_answer}")
        else:
            thinking_process = "æ€è€ƒè¿‡ç¨‹å·²éšè—"
            print(f"ğŸ’¡ å›ç­”ï¼š{final_answer}")
        
        updated_history = history + [
            {"role": "user", "content": prompt},
            {"role": "assistant", "content": final_answer}
        ]
        
        print("âœ… å¤šæ¨¡æ€å¯¹è¯å®Œæˆï¼")
        
        return (final_answer, thinking_process, updated_history)

# èŠ‚ç‚¹æ³¨å†Œ
NODE_CLASS_MAPPINGS = {
    "Qwen3VL_TextChat": Qwen3VL_TextChat,
    "Qwen3VL_MultimodalChat": Qwen3VL_MultimodalChat,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "Qwen3VL_TextChat": "Qwen3-VL æ–‡æœ¬å¯¹è¯",
    "Qwen3VL_MultimodalChat": "Qwen3-VL å¤šæ¨¡æ€å¯¹è¯"
}
